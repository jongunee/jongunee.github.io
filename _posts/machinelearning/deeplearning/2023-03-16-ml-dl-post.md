---
layout: post
title: 인공 신경망의 최적화
description: >
  [참조] 인프런 - 실전 인공지능으로 이어지는 딥러닝 개념 잡기
sitemap: true
hide_last_modified: true
categories:
  - machinelearning
  - deeplearning
---

# 인공 신경망의 최적화

* toc
{:toc .large-only}

## 하강법

![그림1](/assets/img/ml/optimization.png)

- 모델의 input을 받아 output을 추출하는 것을 예측이라고 함
- Loss Function은 실제 값과 예측 값의 차이를 나타내는 척도이며 차이가 작을 수록 학습이 잘 되고 있다는 것
- 목표는 그림에서 Loss 값을 작게 만드는 $$\mathbf{w}$$를 찾는 것

따라서 $$minL(y, \hat{y})$$을 목적 함수라고 칭함


![그림2](/assets/img/ml/descent_method.png)

> 하강법이란 원래 지점보다 더 낮은 $$\mathbf{w}$$를 찾는 방법을 의미. $$\mathbf{w}$$ 지점이 내려가는 방향과 접선의 기울기는 항상 반대이기 때문에 다음 부등호를 만족하면 됨

$$\nabla f(w)^T\Delta w < 0$$

- 초기 위치에 따라서 도착 지점이 달라질 수 있다는 단점이 있음  
ex) 3, 4차 그래프

## 경사 하강법

**식 유도**

$$w \;\leftarrow w \; + \; \mu \Delta w, \; \nabla f(w)^T\Delta w < 0 $$  
$$\Delta w \; = \; -\nabla f(w)$$  
$$\nabla f(w)^T(-\nabla f(w)) \; = \; -\nabla f(w)^T \nabla f(w) \; < \; 0$$  
$$w\;\leftarrow \; w \; - \mu \nabla f(w)$$


**경사 하강법 적용**

$$\mu = 0.4$$ 대입


![그림1](/assets/img/ml/gradient_descent_1.png)

$$\mu = 0.6$$ 대입

![그림2](/assets/img/ml/gradient_descent_2.png)

$$\mu = 1.2$$ 대입

![그림3](/assets/img/ml/gradient_descent_3.png)

위 그림들 처럼 $$\mu$$ 값이 너무 크면 수렴하지 않을 수 있고 $$\mu$$ 값이 너무 작으면 수렴 속도가 너무 오래 걸릴 수 있기 때문에 적절한 $$\mu$$ 값을 찾아야 한다.

## 확률적 경사 하강법과 최적화 기법

**SGD(Stochastic Gradient Descent)**

![그림3](/assets/img/ml/sgd.png)

경사 하강법에서는 데이터가 엄청 큰 상황에서 알고리즘에 문제가 없더라도 하드웨어적인 문제로 동작이 잘 안될 수 있다. 이를 보완해서 나온 것이 바로 <span style='background-color: #f5f0ff'>SGD</span>

- 데이터 전체를 입력 값으로 넣는게 아니라 데이터를 쪼개서 무작위로 mini-batch 형태로 하나씩 넣어줌








<span style="font-size:70%">[참조] 인프런 - 실전 인공지능으로 이어지는 딥러닝 개념 잡기

끝!
